# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/time/01_base.ipynb.

# %% auto 0
__all__ = ['TimeDataset', 'TimeDataModule']

# %% ../../nbs/time/01_base.ipynb 3
import numpy as np, pandas as pd

import torch, torch.nn as nn, pytorch_lightning as pl
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence

# %% ../../nbs/time/01_base.ipynb 4
from dataclasses import dataclass, field, KW_ONLY
from beartype.typing import Optional, Union, Iterable

from iza.static import TIME, SERIES
from iza.utils import Slice

# %% ../../nbs/time/01_base.ipynb 5
from littyping.core import (Device)

from ..abc.dfdm.base import set_dataset, BaseDataFrameDataModule
from ..abc.dfds.base import BaseDataFrameDataset
from litds.types import (
    SequenceWithLength, SequencesWithLengths
)
from ..mocks.time import MockTimeSeries
from .mixs import TimeDatasetMixin

# %% ../../nbs/time/01_base.ipynb 8
@dataclass
class TimeDataset(TimeDatasetMixin, BaseDataFrameDataset): 
    pass

# %% ../../nbs/time/01_base.ipynb 10
@set_dataset(TimeDataset)
class TimeDataModule(BaseDataFrameDataModule):
    time_key: str = TIME
    _: KW_ONLY = field(default=None, init=False)
    batch_size: Optional[int] = 64
    include_time: Optional[bool] = False
    device: Optional[Device] = None

    def setup(self, stage:Optional[str]=None):
        pass

    def train_dataloader(self):
        ds = self.make_dataset()
        self.train_ds = ds
        return DataLoader(ds, batch_size=self.batch_size, collate_fn=self.collate_fn)

    def collate_fn(self, batch):
        seqs, time = zip(*batch)
        # seqs = pad_sequence(seqs, batch_first=True)
        seqs = torch.stack(seqs)
        time = torch.stack(time)
        return seqs, time

    def getall(self, pad:Optional[bool]=True) -> Union[SequenceWithLength, SequencesWithLengths]:
        pad = getattr(self, 'pad', pad)
        return self.ds.getall(pad=pad)
