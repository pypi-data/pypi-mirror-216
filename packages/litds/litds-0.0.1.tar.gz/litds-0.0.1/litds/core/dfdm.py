# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/core/01_dfdm.ipynb.

# %% auto 0
__all__ = ['DataFrameDataModule']

# %% ../../nbs/core/01_dfdm.ipynb 3
import os, math, inspect
import numpy as np, pandas as pd
from dataclasses import dataclass, field, KW_ONLY

import torch, torch.nn as nn, pytorch_lightning as pl
from torch.utils.data import Dataset, DataLoader

from beartype.typing import Optional

from iza.static import (LABEL, )
from littyping.core import Device

# %% ../../nbs/core/01_dfdm.ipynb 4
from ..abc.dfdm.base import BaseDataFrameDataModule, set_dataset
from .dfds import DataFrameDataset
from ..mocks.time import MockTimeSeries

# %% ../../nbs/core/01_dfdm.ipynb 6
@set_dataset(DataFrameDataset)
@dataclass
class DataFrameDataModule(BaseDataFrameDataModule):
    label_key: str = LABEL

    _: KW_ONLY = field(default=None, init=False)
    batch_size: Optional[int] = 64
    include_time: Optional[bool] = False
    device: Optional[Device] = None

    def setup(self, stage: Optional[str] = None):
        pass

    def train_dataloader(self):
        ds = self.make_dataset(df=self.df)
        self.train_ds = ds
        return DataLoader(ds, batch_size=self.batch_size, collate_fn=self.collate_fn)

    def collate_fn(self, batch):
        samples, targets = zip(*batch)        
        samples = torch.stack(samples)
        targets = torch.stack(targets)
        return samples, targets

    def getall(self, pad:Optional[bool]=True):
        return self.ds.getall(pad=pad)
