#include <string>
std::string python_script = 
"import torch\n"
"from torch.nn import Module, BatchNorm1d, LeakyReLU, Conv1d, ModuleList, Softmax, Sigmoid, Flatten, Dropout2d, Linear\n"
"from torch.optim.lr_scheduler import LambdaLR\n"
"from torch.optim import Optimizer\n"
"from torch.utils.data import Dataset, DataLoader\n"
"import argparse\n"
"import math\n"
"import random\n"
"import numpy as np\n"
"import warnings\n"
"from progress.bar import Bar\n"
"warnings.filterwarnings('ignore')\n"
"parser = argparse.ArgumentParser(description='SPLAM! splice junction prediction.')\n"
"parser.add_argument('-f', metavar='<junction.fa>', required=True, help='the junction FASTA file in SPLAM! format')\n"
"parser.add_argument('-o', metavar='<score.bed>', required=True, help='the output SPLAM! scores for junctions')\n"
"parser.add_argument('-m', metavar='<model.pt>', required=True, help='the path to the SPLAM! model')\n"
"args = parser.parse_args()\n"
"JUNC_FA = args.f\n"
"OUT_SCORE = args.o\n"
"MODEL_PATH = args.m\n"
"CARDINALITY_ITEM = 16\n"
"SEQ_LEN = 800\n"
"JUNC_START = 200\n"
"JUNC_END = 600\n"
"IN_MAP = np.asarray([[0, 0, 0, 0],\n"
"                     [1, 0, 0, 0],\n"
"                     [0, 1, 0, 0],\n"
"                     [0, 0, 1, 0],\n"
"                     [0, 0, 0, 1]])\n"
"OUT_MAP = np.asarray([[1, 0, 0],\n"
"                      [0, 1, 0],\n"
"                      [0, 0, 1],\n"
"                      [0, 0, 0]])\n"
"class ResidualUnit(Module):\n"
"    def __init__(self, l, w, ar, bot_mul=1):\n"
"        super().__init__()\n"
"        bot_channels = int(round(l * bot_mul))\n"
"        self.batchnorm1 = BatchNorm1d(l)\n"
"        self.relu = LeakyReLU(0.1)\n"
"        self.batchnorm2 = BatchNorm1d(l)\n"
"        self.C = bot_channels//CARDINALITY_ITEM\n"
"        self.conv1 = Conv1d(l, l, w, dilation=ar, padding=(w-1)*ar//2, groups=self.C)\n"
"        self.conv2 = Conv1d(l, l, w, dilation=ar, padding=(w-1)*ar//2, groups=self.C)\n"
"    def forward(self, x, y):\n"
"        x1 = self.relu(self.batchnorm1(self.conv1(x)))\n"
"        x2 = self.relu(self.batchnorm2(self.conv2(x1)))\n"
"        return x + x2, y\n"
"class Skip(Module):\n"
"    def __init__(self, l):\n"
"        super().__init__()\n"
"        self.conv = Conv1d(l, l, 1)\n"
"    def forward(self, x, y):\n"
"        return x, self.conv(x) + y\n"
"class SpliceNN(Module):\n"
"    def __init__(self, L=64, W=np.array([11]*8+[21]*4+[41]*4), AR=np.array([1]*4+[4]*4+[10]*4+[25]*4)):\n"
"        super().__init__()\n"
"        self.CL = 2 * (AR * (W - 1)).sum()  # context length\n"
"        self.conv1 = Conv1d(4, L, 1)\n"
"        self.skip1 = Skip(L)\n"
"        self.residual_blocks = ModuleList()\n"
"        for i, (w, r) in enumerate(zip(W, AR)):\n"
"            self.residual_blocks.append(ResidualUnit(L, w, r))\n"
"            if (i+1) % 4 == 0:\n"
"                self.residual_blocks.append(Skip(L))\n"
"        if (len(W)+1) % 4 != 0:\n"
"            self.residual_blocks.append(Skip(L))\n"
"        self.last_cov = Conv1d(L, 3, 1)\n"
"        self.flatten = Flatten()\n"
"        self.drop_out = Dropout2d(0.2)\n"
"        self.fc = Linear(2400, 1)\n"
"        self.softmax = Softmax(dim=1)\n"
"        self.sigmoid = Sigmoid()\n"
"    def forward(self, x):\n"
"        x, skip = self.skip1(self.conv1(x), 0)\n"
"        for m in self.residual_blocks:\n"
"            x, skip = m(x, skip)\n"
"        output = self.sigmoid(self.fc(self.flatten(self.last_cov(skip))))\n"
"        return output\n"
"def get_donor_acceptor_scores(D_YL, A_YL, D_YP, A_YP):\n"
"    return D_YL[:, 200], D_YP[:, 200], A_YL[:, 600], A_YP[:, 600]\n"
"           \n"
"def one_hot_encode(Xd, Yd):\n"
"    return IN_MAP[Xd.astype('int8')], [OUT_MAP[Yd[t].astype('int8')] for t in range(1)]\n"
"      \n"
"def create_datapoints(seq, strand):\n"
"    # seq = 'N'*(CL_MAX//2) + seq + 'N'*(CL_MAX//2)\n"
"    seq = seq.upper().replace('A', '1').replace('C', '2')\n"
"    seq = seq.replace('G', '3').replace('T', '4').replace('N', '0').replace('K', '0').replace('R', '0')\n"
"    jn_start = JUNC_START\n"
"    jn_end = JUNC_END\n"
"    #######################################\n"
"    # predicting pb for every bp\n"
"    #######################################\n"
"    X0 = np.asarray(list(map(int, list(seq))))\n"
"    Y0 = [np.zeros(SEQ_LEN) for t in range(1)]\n"
"    if strand == '+':\n"
"        for t in range(1):        \n"
"            Y0[t][jn_start] = 2\n"
"            Y0[t][jn_end] = 1\n"
"    X, Y = one_hot_encode(X0, Y0)\n"
"    return X, Y\n"
"def get_cosine_schedule_with_warmup(\n"
"      optimizer: Optimizer,\n"
"      num_warmup_steps: int,\n"
"      num_training_steps: int,\n"
"      num_cycles: float = 0.5,\n"
"      last_epoch: int = -1,\n"
"    ):\n"
"    def lr_lambda(current_step):\n"
"        # Warmup\n"
"        if current_step < num_warmup_steps:\n"
"            return float(current_step) / float(max(1, num_warmup_steps))\n"
"        # decadence\n"
"        progress = float(current_step - num_warmup_steps) / float(\n"
"          max(1, num_training_steps - num_warmup_steps)\n"
"        )\n"
"        return max(\n"
"          0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n"
"        )\n"
"    return LambdaLR(optimizer, lr_lambda, last_epoch)\n"
"def get_accuracy(y_prob, y_true):\n"
"    assert y_true.ndim == 1 and y_true.size() == y_prob.size()\n"
"    y_prob = y_prob > 0.5\n"
"    return (y_true == y_prob).sum().item() / y_true.size(0)\n"
"def model_fn(DNAs, labels, model, criterion):\n"
"    outs = model(DNAs)\n"
"    loss = categorical_crossentropy_2d(labels, outs, criterion)\n"
"    return loss, outs\n"
"def categorical_crossentropy_2d(y_true, y_pred, criterion):\n"
"    SEQ_WEIGHT = 5\n"
"    gamma = 2\n"
"    return - torch.mean(y_true[:, 0, :] * torch.mul( torch.pow( torch.sub(1, y_pred[:, 0, :]), gamma ), torch.log(y_pred[:, 0, :]+1e-10) )\n"
"                        + SEQ_WEIGHT * y_true[:, 1, :] * torch.mul( torch.pow( torch.sub(1, y_pred[:, 1, :]), gamma ), torch.log(y_pred[:, 1, :]+1e-10) )\n"
"                        + SEQ_WEIGHT * y_true[:, 2, :] * torch.mul( torch.pow( torch.sub(1, y_pred[:, 2, :]), gamma ), torch.log(y_pred[:, 2, :]+1e-10) ))\n"
"def split_seq_name(seq):\n"
"    return seq[1:]\n"
"class myDataset(Dataset):\n"
"    def __init__(self, type, of, shuffle, segment_len=800):\n"
"        self.segment_len = segment_len\n"
"        self.data = []\n"
"        self.indices = []\n"
"        pidx = 0\n"
"        with open(of, 'r') as f:\n"
"            lines = f.read().splitlines()\n"
"            seq_name = ''\n"
"            seq = ''\n"
"            for line in lines:\n"
"                if pidx % 2 == 0:\n"
"                    seq_name = split_seq_name(line)\n"
"                elif pidx % 2 == 1:\n"
"                    seq = line\n"
"                    if seq[0] == '>':\n"
"                        seq_name = line\n"
"                        continue\n"
"                    X, Y = create_datapoints(seq, '+')\n"
"                    X = torch.Tensor(np.array(X))\n"
"                    Y = torch.Tensor(np.array(Y)[0])\n"
"                    if X.size()[0] != 800:\n"
"                        print('seq_name: ', seq_name)\n"
"                        print(X.size())\n"
"                        print(Y.size())\n"
"                    self.data.append([X, Y, seq_name])\n"
"                pidx += 1\n"
"                if pidx %100000 == 0:\n"
"                    print('\t', pidx, ' junctions loaded.')\n"
"        index_shuf = list(range(len(self.data)))\n"
"        if shuffle:\n"
"            random.shuffle(index_shuf)\n"
"        list_shuf = [self.data[i] for i in index_shuf]\n"
"        self.data = list_shuf \n"
"        self.indices = index_shuf\n"
"        print('\t', pidx, ' junctions loaded.')\n"
"    def __len__(self):\n"
"        return len(self.data)\n"
"    def __getitem__(self, index):\n"
"        feature = self.data[index][0]\n"
"        label = self.data[index][1]\n"
"        seq_name = self.data[index][2]\n"
"        feature = torch.flatten(feature, start_dim=1)\n"
"        return feature, label, seq_name\n"
"def get_dataloader(batch_size, n_workers, output_file, shuffle, repeat_idx):\n"
"    testset = myDataset('test', output_file, shuffle, SEQ_LEN)\n"
"    test_loader = DataLoader(\n"
"        testset,\n"
"        batch_size = batch_size,\n"
"        drop_last = False,\n"
"        pin_memory = True,\n"
"    )\n"
"    return test_loader\n"
"def test_model():\n"
"    BATCH_SIZE = 100\n"
"    N_WORKERS = None\n"
"    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
"    print(f'[Info] Loading model ...',flush = True)\n"
"    model = torch.jit.load(MODEL_PATH)\n"
"    model = model.to('cpu')\n"
"    print(f'[Info] Done loading model',flush = True)\n"
"    print(f'[Info] Loading data ...',flush = True)\n"
"    test_loader = get_dataloader(BATCH_SIZE, N_WORKERS, JUNC_FA, False, str(0))\n"
"    print(f'[Info] Done loading data ...',flush = True)\n"
"    criterion = torch.nn.BCELoss()\n"
"    fw_junc_scores = open(OUT_SCORE, 'w')\n"
"    model.eval()\n"
"    junc_counter = 0    \n"
"    pbar = Bar('[Info] SPLAM! ', max=len(test_loader))\n"
"    with torch.no_grad():\n"
"        for batch_idx, data in enumerate(test_loader):\n"
"            # print('batch_idx: ', batch_idx)\n"
"            # DNAs:  torch.Size([40, 800, 4])\n"
"            # labels:  torch.Size([40, 1, 800, 3])\n"
"            DNAs, labels, seqname = data \n"
"            DNAs = DNAs.to(torch.float32).to(device)\n"
"            labels = labels.to(torch.float32).to(device)\n"
"            DNAs = torch.permute(DNAs, (0, 2, 1))\n"
"            labels = torch.permute(labels, (0, 2, 1))\n"
"            loss, yp = model_fn(DNAs, labels, model, criterion)\n"
"            is_expr = (labels.sum(axis=(1,2)) >= 1)\n"
"            A_YL = labels[is_expr, 1, :].to('cpu').detach().numpy()\n"
"            A_YP = yp[is_expr, 1, :].to('cpu').detach().numpy()\n"
"            D_YL = labels[is_expr, 2, :].to('cpu').detach().numpy()\n"
"            D_YP = yp[is_expr, 2, :].to('cpu').detach().numpy()\n"
"            donor_labels, donor_scores, acceptor_labels, acceptor_scores = get_donor_acceptor_scores(D_YL, A_YL, D_YP, A_YP)\n"
"            for idx in range(len(yp)):\n"
"                chr, start, end, strand, aln_num = seqname[idx].split(';')\n"
"                if strand == '+':\n"
"                    fw_junc_scores.write(chr + '\t'+ str(start) + '\t' + str(end) + '\tJUNC_' + str(junc_counter) + '\t' + str(aln_num) + '\t'+ strand + '\t' + str(donor_scores[idx]) + '\t' + str(acceptor_scores[idx]) + '\\n')\n"
"                elif strand == '-':\n"
"                    fw_junc_scores.write(chr + '\t'+ str(end) + '\t' + str(start) + '\tJUNC_' + str(junc_counter) + '\t' + str(aln_num) + '\t'+ strand+ '\t' + str(donor_scores[idx]) + '\t' + str(acceptor_scores[idx]) + '\\n')\n"
"                junc_counter += 1\n"
"    pbar.finish()\n"
"    fw_junc_scores.close()\n"
"if __name__ == '__main__':\n"
"    test_model()\n";
