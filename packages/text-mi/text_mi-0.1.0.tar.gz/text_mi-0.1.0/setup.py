# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['mi']

package_data = \
{'': ['*']}

install_requires = \
['numpy>1.23', 'pandas>1.4', 'toolz>=0.12.0,<0.13.0', 'tqdm>4.64.0']

setup_kwargs = {
    'name': 'text-mi',
    'version': '0.1.0',
    'description': 'MI calculations based on C extension for python and Pandas.',
    'long_description': '# Description\nC library for python with custom mutual information calculations. Originally was part of https://github.com/volkoshkursk/NB.\n\nThere are two approach to calculate the MI: based on the Pandas framework and based on the custom C library. Pandas-based approach is faster, however you need to construct a DataFrame with columns ``` doc_id target token ``` where `token` is the column with only one token in the row. If you have a much data it can be memory-expensive operation. \n\nFrom the other hand, the custom library - based approach does not need in some special dataframe. It needs only two iterables with same lengthes: the targets and the textes. However, it is slower than the pandas-based approach.\n\n# Requirements\nIf you want to build by `make` you need to have `make` and `python-dev` packages installed.\nIt is recommended to have a poetry library.\n# Installation\n\nThere is several different ways to install this library:\n\n## Installation with poetry.\nThis is a simpliest and recommended way to install the library.\nJust run \n```\npoetry install\n``` \nåto install the library. Then you can run your code in poetry\'s virtual environment with \n```\npoetry run script.py\n```\n\nwhere `script.py` is your script.\n\n## Building from sources.\nThis way is useful if you don\'t have a poetry by some reason. However, it is strongly recommended to use the poetry installation.\n\nYou need create libmi.so to run this project.\nFor this purpose you need installed python3-dev on your system. For building on library *nix system run\n\n```\nmake clean && make\n```\n\nDefault Python version is 3.8. If you need build for another version run \n\n```\nmake clean && make VER=<ver>\n```\n\nwhere instead `<ver>` should be your version.\n\nAlso, you need to install Python dependencies with \n```\npip install -r requirements.txt\n```\n\n# Theory\n\nMI is a non-negative value. It is equal to 0 when random variables are independent.\n\n The main formula, using for calculating MI was found in book Manning K.D., Raghavan P., Schütze H. Introduction to information search. \n\n$$\n\\begin{align}\nI(U,C) = {N_{11} \\over N}  \\log_2\\bigl({N_{11}N \\over{(N_{10} + N_{11}) (N_{11} + N_{01})}}\\bigr) + \\\\\n{N_{01} \\over N} \\log_2({{N_{01} N} \\over {(N_{00} + N_{01}) (N_{11} + N_{01})}}) + \\\\\n{N_{10} \\over N} \\log_2({{N_{10} N} \\over {(N_{10} + N_{11}) (N_{10} + N_{00})}}) + \\\\\n{N_{00} \\over N} \\log_2({{N_{00} N} \\over {(N_{00} + N_{01}) (N_{10} + N_{00})}}),\n\\end{align}\n$$\n\nwhere \n- $U$ is the random variable, shows presence of the word W.\n- $C$ is the random variable, shows presence of the theme T.\n- $N$ is the number of all docs in dataset,\n- $N_{11}$ is the number of documents which have word W and theme T,\n- $N_{10}$ is the number of documents which have word W and haven\'t theme T,\n- $N_{01}$ is the number of documents which haven\'t word W and have theme T,\n- $N_{00}$ is the number of documents which haven\'t word W and theme T.    \n\nAs we can see, the formula above can be calculated only if $N_{11}N_{00}N_{01}N_{11} > 0$. When we have lots of themes and less of documents this becomes a problem: for large number of word we can not calculate MI. \n\nThis problem can be solved by going to the limit in each summand in formaula above. However, the result will be different with precise formula even if the formula can be calculated.\n\n# Differences between approaches\n\nAs it was told upper, there is two approaches to mutual information calculation. [First of them](#c-based-approach) is based on custom C extension. It is slower than another one, but require less memory for running. [Second of them](#pandas-based-approach) based on Pandas framework. It is faster than the C-based approach, but requires a lot of memory. \n\nThe another difference between the approaches is in handling division-by-zero exception.\n\nIn Pandas-based approach if $N_{11}N_{00}N_{01}N_{11} = 0$ the result is equal to $0$. In C-extension based approach if $N_{11}N_{00}N_{01} = 0$ the result is calulated as a sum of the summand, where logarithm can be calculated.\n\n# Using\n## C-based approach\nThere are two function you can use: `mi` and `multi_mi`. The first one calculate MI for one word and one theme. The second one uses `mi` for build a dictionary for all words and all themes.\n\n### `mi`\n\n__Args:__\n- ___txts___ (_Iterable[str]_): Iterable of texts.\n- ___tgts___ (_Iterable[str]_): Iterable of targets corresponding to texts from ___txts___. All targets corresponding to one text should be separated by "|".\n- ___word___ (_str_): Word for building Mutual Information for.\n- ___tgt___ (_str_): Target for building mutual information for.\n\n__Result__ (_float_): Mutual Information between ___word___ and ___tgt___.\n\n### `multi_mi`\n\n__Args:__\n- ___txts___ (_Iterable[str]_): Iterable of texts.\n- ___tgts___ (_Iterable[str]_): Iterable of targets corresponding to texts from ___txts___. All targets corresponding to one text should be separated by "|".\n- ___targets____ (_Iterable[str]_): Iterable of targets.\n- ___vocabulary___ (_Iterable[str]_): list of words.\n- ___workers___ (_int_, optional): The number of parallel processes. Defaults to 16.\n\n__Result__ (_dict[dict[float]]_): dictionary of dictionaries with MI as values.\n\nExample of result:\n\n    {\n        \'quick\': {\n                    \'target_1\': 0.1,\n                    \'target_2\': 0.2,\n                    \'target_3\': 0.3\n                 },\n        \'brown\': {\n                    \'target_1\': 0.1,\n                    \'target_2\': 0.2,\n                    \'target_3\': 0.3\n                 },\n        \'fox\': {\n                    \'target_1\': 0.1,\n                    \'target_2\': 0.2,\n                    \'target_3\': 0.3\n                 },\n        \'jumps\': {\n                    \'target_1\': 0.1,\n                    \'target_2\': 0.2,\n                    \'target_3\': 0.3\n                 },\n        \'over\': {\n                    \'target_1\': 0.1,\n                    \'target_2\': 0.2,\n                    \'target_3\': 0.3\n                 },\n        \'the\': {\n                    \'target_1\': 0.1,\n                    \'target_2\': 0.2,\n                    \'target_3\': 0.3\n                 },\n        \'lazy\': {\n                    \'target_1\': 0.1,\n                    \'target_2\': 0.2,\n                    \'target_3\': 0.3\n                 },\n        \'dog\': {\n                    \'target_1\': 0.1,\n                    \'target_2\': 0.2,\n                    \'target_3\': 0.3\n                 },\n    }\n\n## Pandas-based approach\n\nFor using Pandas-based approach you should call the function `mutual_info_pipeline`, which generate a function in which you should send a Pandas.Dataframe, which has follow structure: it has some item identifyer (for example id of document), word and target. This DataFrame can content some other column, but this three is necessary.\n\nThe result will contain follow columns: \n- *target* - name of target,\n- *tokens* - the word,\n- *N11* - the number of documents which have both the word and the theme,\n- *NT* - the number of documents which have the theme,\n- *N1* - the number of documents which have the word,\n- *N* - the number of all docs in dataset, which haven\'t the theme,\n- *N10* - the number of documents which have the word and haven\'t the theme,\n- *N01* - the number of documents which haven\'t the word and have the theme,\n- *N00* - the number of documents which haven\'t both the word and the theme,\n- *N0*  - the number of documents which haven\'t the word,\n- *mutual_information* - MI for the word and the theme.\n\n### `mutual_info_pipeline`\n__Args:__\n- ___target___ (_str_): target columne name.\n- ___tokens___ (_str_): tokens column name.\n- ___doc_id___ (_str_): item id column name.\n- ___docs_number___ (_int_): number of docs.\n\n__Result__ (_Callable[[DataFrame[TokenFrame]], DataFrame[TokensMIFrame]]_): compute pipeline.\n\nExample of using:\n\n```\nfrom mi import mutual_info_pipeline\n\nmutual_info_pipeline(\n    \'target\',\n    \'word\',\n    \'item_id\',\n    docs_number,\n)(df)\n```',
    'author': 'volkoshkursk',
    'author_email': 'zaz-pagani@inbox.ru',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'https://github.com/volkoshkursk/MI',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>3.8',
}
from build import *
build(setup_kwargs)

setup(**setup_kwargs)
