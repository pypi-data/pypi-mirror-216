# Prompts for Cohere's Command models. This works reasonably well but there is room for improvement.
prompts:
  - task: generate_user_intent
    models:
      - cohere/command
      - cohere/command-light
      - cohere/command-light-nightly
    content: |-
      """
      {general_instruction}
      Your task is to generate a short summary called canonical form for the last user utterance.
      """

      # This is how a conversation between a user and the bot can go:
      {sample_conversation}

      # This how the user talks, use these examples to generate the canonical form:
      {examples}

      # This is the current conversation between the user and the bot:
      {sample_conversation_two_turns}
      {history}#generate canonical form

  - task: generate_next_steps
    models:
      - cohere/command
      - cohere/command-light
      - cohere/command-light-nightly
    content: |-
      """
      {general_instruction}
      """

      # This is how a conversation between a user and the bot can go:
      {sample_conversation}

      # This how the bot thinks, use these examples to generate the bot canonical form:
      {examples}

      # This is the current conversation between the user and the bot:
      {sample_conversation_two_turns}
      {history}#generate bot canonical form

  - task: generate_bot_message
    models:
      - cohere/command
      - cohere/command-light
      - cohere/command-light-nightly
    content: |-
      """
      {general_instruction}
      """

      # This is how a conversation between a user and the bot can go:
      {sample_conversation}

      # This is some additional context:
      ```markdown
      {relevant_chunks}
      ```

      # This how the bot talks:
      {examples}

      # This is the current conversation between the user and the bot:
      {sample_conversation_two_turns}
      {history}#generate relevant response based on canonical form
