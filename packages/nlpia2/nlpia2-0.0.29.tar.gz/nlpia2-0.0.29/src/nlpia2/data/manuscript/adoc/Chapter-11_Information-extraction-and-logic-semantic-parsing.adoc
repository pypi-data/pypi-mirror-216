= Information extraction and logic (semantic parsing)
:chapter: 11
:part: 3
:secnums:
:imagesdir: .
:xrefstyle: short
:figure-caption: Figure {chapter}.
:listing-caption: Listing {chapter}.
:table-caption: Table {chapter}.
:stem: latexmath

This chapter covers

//// 
References:
* Stanford KG Course by Vinay: http://www.web.stanford.edu/~vinayc/kg/notes/How_Do_Users_Interact_With_a_Knowledge_Graph.html
* recent tutorial on towards data science: https://12ft.io/proxy?&q=https%3A%2F%2Ftowardsdatascience.com%2Fnlp-with-python-knowledge-graph-12b93146a458
* Knowledge Representation Learning python library with state of the art performance on academic benchmarks: https://github.com/thunlp/OpenKE

A summary sentence for each subsection and a way to summarize all of that in the beginning:

// SUM: a few sentences

Dependency parsing and Constituency parsing
    # Brief intro, and why did it come up
    # Current benchmarks and baselines
    # Why is it important and what are the practical applications (Relation extraction, semantic parsing)
    # Rule based algorithms 
    # Why neural networks are much better at it?
    # Current state of the art methods and the available open source platforms 

# Discourse parsing
    # Sense making


# Relation extraction

# Discourse segmentation
# Semantic parsing using AST and Discourse
# Coreference resolution

////

* Extracting numerical and math expressions from text
* Tagging tokens with their part-of-speech (POS)
* Parsing text to create a dependency tree
* Using named entity recognition algorithms for information extraction
* Using transformers to create a knowledge base from text
* Querying the Wikidata knowledge base for fact checking 

In the last chapter you learned how to use transformers to generate smart sounding words.
Now you're ready to learn how to generate genuinely intelligent text.
To do this you need to know how to extract knowledge from text.
This can help fill the _common sense knowledge_ gap in large language models.
To fill that gap you will store that extracted knowledge in a data structure called a _knowledge graph_.
Your machines and algorithms can programmatically query a knowledge graph for facts about the world
A _knowledge base_ is a database management system that stores knowledge efficiently and gives you query language similar to SQL which you can use ask questions about the knowledge in the data base. 

Once you have a knowledge base, you can use it to programmatically generate logical statements that make sense because they logically follow from the knowledge you stored in your knowledge base.
You can even use it to infer new facts or _logical inferences_ about the world.
This is why you may hear people call forward propagation "inference," because researchers are hoping that it will some day match human's ability to logically infer things about the world.
But you're not going to use deep learning for logical inference this time, you're going to use the tried and true approach called "symbolic reasoning."

Your chatbots and AI agents will now have a way to correctly reason about the world in an explainable way that agrees with the knowledge you've given it.
This is called symbolic reasoning as opposed to the probabilistic reasoning of deep learning.
It's also called GOFAI (Good Old Fashioned AI), and it's back in fashion as researchers struggle to build general intelligence that is grounded in reality.
Besides helping ground in factual knowledge, you will now be able to have your AI explain its reasoning in a valid way, instead of just making things up. 

== Named entities and the connections between them

You'd like your machine to extract pieces of information and facts from text so it can know a little bit about what a user is saying.
For example, imagine a user says "Remind me to read aiindex.org on Monday."
You'd like that statement to trigger a calendar entry or alarm for the next Monday after the current date.
Easier said that done.

To trigger correct actions with natural language you need something an NLU pipeline or parser that is little less fuzzy than a transformer or large language model.
You need to know that "me" represents a particular kind of _named entity_: a person.
Named entities are natural language terms or n-grams that refer to a particular thing in the real world, for example a person, place or thing.
Sound familiar?
In English grammar, the _part of speech_ (POS) for a person, place or thing is "noun".
So you'll see that the POS tag that SpaCy associates with the tokens for a named entity is "NOUN".
And the SpaCy package knows how to recognize the relationships (dependencies) between words and phrases.
Once you have a _dependency tree_ of the hierarchy of grammatical relationships between words you have a way to process the logical meaning of a sentence. 

And the chatbot should know that it can expand or _resolve_ that word by replacing it with that person's username or other identifying information.
You'd also need your chatbot to recognize that "aiindex.org" is an abbreviated URL, a named entity, the name of a specific instance of something, like a website or company.
And you need to know that a normalized spelling of this particular kind of named entity might be "http://aiindex.org", "https://aiindex.org", or maybe even "https://www.aiindex.org".
Likewise you need your chatbot to recognize that Monday is one of the days of the week (another kind of named entity called an "event") and be able to find it on the calendar.

For the chatbot to respond properly to that simple request, you also need it to extract the relation between the named entity "me" and the command "remind."
You'd even need to recognize the implied subject of the sentence, "you", referring to the chatbot, another person named entity.
And you need to teach the chatbot that reminders happen in the future, so it should find the soonest upcoming Monday to create the reminder.

A typical sentence may contain several named entities of various types, such as geographic entities, organizations, people, political entities, times (including dates), artifacts, events, and natural phenomena.
And a sentence can contain several relations, too -- facts about the relationship between the named entities in the sentence.

=== A knowledge base

Besides just extracting information from the text of a user statement, you can also use information extraction to help your chatbot train itself!
If you have your chatbot run information extraction on a large corpus, such as Wikipedia, that corpus will produce facts about the world that can inform future chatbot behaviors and replies.
Some chatbots record all the information they extract (from offline reading-assignment "homework") in a knowledge base.
That knowledge base can later be queried to help your chatbot make informed decisions or inferences about the world.

Chatbots can also store knowledge about the current user "session" or conversation.
Knowledge that is relevant only to the current conversation is called "context."
This contextual knowledge can be stored in the same global knowledge base that supports the chatbot, or it can be stored in a separate knowledge base.
Commercial chatbot APIs, such as IBM's Watson or Amazon's Lex, typically store context separate from the global knowledge base of facts that it uses to support conversations with all the other users.

Context can include facts about the user, the chatroom or channel, or the weather and news for that moment in time.
Context can even include the changing state of the chatbot itself, based on the conversation.
A smart chatbot keeps track of self-knowledge or subjective knowledge with the same gusto that it manages objective knowledge.
An example of self-knowledge is the history of all the things the chatbot has already said to someone, such as the questions it has already asked of the user. That way it won't repeat itself.

So that's the goal for this chapter, teaching your bot to understand what it reads.
And you'll put that understanding into a flexible data structure designed to store knowledge.
Then your bot can use that knowledge to make decisions and say smart stuff about the world.

In addition to the simple task of recognizing numbers and dates in text, you'd like your bot to be able to extract more general information about the world.
And you'd like it to do this on its own, rather than having you "program" everything you know about the world into it.
For example, you'd like it to be able to learn from natural language documents such as this sentence from Wikipedia:

_In 1983, Stanislav Petrov, a lieutenant colonel of the Soviet Air Defense Forces, saved the world from nuclear war._

If you were to take notes in a history class after reading or hearing something like that, you'd probably paraphrase things and create connections in your brain between concepts or words.
You might reduce it to a piece of knowledge, that thing that you "got out of it."
You'd like your bot to do the same thing.
You'd like it to "take note" of whatever it learns, such as the fact or knowledge that Stanislov Petrov was a lieutenant colonel.
This could be stored in a data structure something like this:

[source,python]
----
('Stanislav Petrov', 'is-a', 'lieutenant colonel')
----

This is an example of two named entity nodes ('Stanislav Petrov' and 'lieutenant colonel') and a relation or connection ('is a') between them in a knowledge graph or knowledge base.
When a relationship like this is stored in a form that complies with the RDF standard (relation description format) for knowledge graphs, it's referred to as an RDF triplet.
Historically these RDF triplets were stored in XML files, but they can be stored in any file format or database that can hold a graph of triplets in the form of `(subject, relation, object)`.

A collection of these triplets is a knowledge graph.
This is also sometimes called an ontology by linguists, because it is storing structured information about words.
But when the graph is intended to represent facts about the world rather than merely words, it is referred to as a knowledge graph or knowledge base.
Figure 11.1 is a graphic representation of the knowledge graph you'd like to extract from a sentence like that.

.Stanislav knowledge graph
image::../images/ch11/Stanislav-Knowledge-Graph.png[Stanislav Knowledge Graph showing 'is-a' and 'is-famous-for' relations extracted, width=80%, link="../images/ch11/Stanislav-Knowledge-Graph.png"]

The red edge and node in this knowledge graph represents a fact that could not be directly extracted from the statement about Stanislav.
But this fact that "lieutenant colonel" is a military rank could be inferred from the fact that the title of a person who is a member of a military organization is a military rank.
This logical operation of deriving facts from a knowledge graph is called knowledge graph _inference_.
It can also be called querying a knowledge base, analogous to querying a relational database.

For this particular inference or query about Stanislov's military ranks, your knowledge graph would have to already contain facts about militaries and military ranks.
It might even help if the knowledge base had facts about the titles of people and how people relate to occupations (jobs).
Perhaps you can see now how a base of knowledge helps a machine understand more about a statement than it could without that knowledge.
Without this base of knowledge, many of the facts in a simple statement like this will be "over the head" of your chatbot.
You might even say that questions about occupational rank would be "above the pay grade" of a bot that only knew how to classify documents according to randomly allocated topics.footnote:[See chapter 4 if you've forgotten about how random topic allocation can be.]

It may not be obvious how big a deal this is, but it is a _BIG_ deal.
If you've ever interacted with a chatbot that doesn't understand "which way is up", literally, you'd understand.
One of the most daunting challenges in AI research is the challenge of compiling and efficiently querying a knowledge graph of common sense knowledge.
We take common sense knowledge for granted in our everyday conversations.

Humans start acquiring much of their common sense knowledge even before they acquire language skill.
We don't spend our childhood writing about how a day begins with light and sleep usually follows sunset.
And we don't edit Wikipedia articles about how an empty belly should only be filled with food rather than dirt or rocks.
This makes it hard for machines to find a corpus of common sense knowledge to read and learn from.
No common-sense knowledge Wikipedia articles exist for your bot to do information extraction on.
And some of that knowledge is instinct, hard-coded into our DNA.footnote:[There are hard-coded common-sense knowledge bases out there for you to build on. Google Scholar is your friend in this knowledge graph search.]

All kinds of factual relationships exist between things and people, such as "kind-of", "is-used-for", "has-a", "is-famous-for", "was-born", and "has-profession."
NELL, the Carnegie Mellon Never Ending Language Learning bot is focused almost entirely on the task of extracting information about the `'kind-of'` relationship.

Most knowledge bases normalize the strings that define these relationships, so that "kind of" and "type of" would be assigned a normalized string or ID to represent that particular relation.
And some knowledge bases also normalize the nouns representing the objects in a knowledge base.
So the bigram "Stanislav Petrov" might be assigned a particular ID.
Synonyms for "Stanislav Petrov", like "S. Petrov" and "Lt Col Petrov", would also be assigned to that same ID, if the NLP pipeline suspected they referred to the same person.

A knowledge base can be used to build a practical type of chatbot called a _question answering system_ (QA system).
Customer service chatbots, including university TA bots, rely almost exclusively on knowledge bases to generate their replies.footnote:[2016, AI Teaching Assistant at GaTech: http://www.news.gatech.edu/2016/05/09/artificial-intelligence-course-creates-ai-teaching-assistant]
Question answering systems are great for helping humans find factual information, which frees up human brains to do the things they're better at, such as attempting to generalize from those facts.
Humans are bad at remembering facts accurately but good at finding connections and patterns between those facts, something machines have yet to master.
We talk more about question answering chatbots in the next chapter.

== Extracting the structure of text
In the previous section, you learned how to recognize and tag named entities in text.
Now you'll learn how to find relationships between these entities.
This can help your NLP pipeline "understand" more complex thoughts or ideas.
NLP researchers have identified two separate problems or models that can be used to identify how the words in a sentence work together to create meaning: _dependency parsing_ and _constituency parsing_.
_Dependency parsing_ will give your NLP pipelines the ability to diagram sentences like you learned to do in grammar school (elementary school).
And these tree data structures give your model a representation of the logic and grammar of a sentence.
This will help your bots become a bit smarter about how they interpret sentences and act on them.

But wait, you're probably wondering why sentence diagrams are so important.
After all, you've probably already forgotten how to create them yourself and have probably never used them in real life.
But that's only because you've internalized this model of the world.
We need to create that understanding in bots so they can be used to do the same things you do without thinking:

- Grammar checkers
- Spell checkers
- Writing coaches
- Translation
- Common sense understanding
- Intent recognition
- Virtual assistants
- Prosocial AI (social intelligence)

Basically, dependency parsing will help your NLP pipelines for all those applications mentioned in Chapter 1... better.
Have you noticed how chatbots like GPT-3 often fall on their face when it comes to understanding simple sentences or having a substantive conversation?
As soon as you start to ask them about the logic or reasoning of the words they are "saying" they stumble.
Chatbot developers and conversation designers get around this limitation by using rule-based chatbots for substantive conversations like therapy and teaching.
The open-ended neural neural network models PalM and GPT-3 are only used when the user tries to talk about something that hasn't yet been programmed into it.
And the language models are trained with the objective of steering the conversation back to something that the bot knows about and has rules for.
Jakub Konrád and his teammates at CTU Prague won the $1M SocialBot prize in 2021 with this approach.footnote:["Alquist 4.0: Towards Social Intelligence Using Generative Models and Dialogue Personalization" (https://arxiv.org/pdf/2109.07968.pdf)]

// show example convo with Mitsuku



Dependency parsing, as the name suggests, relies on "dependencies" between the words in a sentence to extract information. 
"Dependencies" between two words could refer to their grammatical, phrasal, or any custom relations. 
But in the context of dependency parse trees, we refer to the grammatical relationships between word pairs of the sentence, one of them acting as the "head" and the other one the "dependent". 
There exists one word in the sentence which isn't dependent on any other word in the parse tree, and this word is called the ROOT.
There are 37 "dependent" relations that a word could possibly have, and these relations are adapted from the *Universal Stanford Dependencies system*.

// show dependency parse image

This technique can be really useful in rule-based information extraction, especially in chatbots. 
Consider the example we used earlier in this chapter: "Remind me to read aiindex.org on Monday." 
Running this sentence through a dependency parser reveals that the relationship between "read" and "aiindex.org" is "Direct Object" and that between "read" and "Monday" is "Prepositional Object". 
How is this information useful to us? 
Let us say the chatbot had to find out what exactly it needs to remind the user to read. 
Examining the "Direct Object" would reveal that it is "aiindex.org" that it needs to remind the user to study. 
Similarly, it can also infer that it needs to do this on Monday.

This way, all the chatbot needs to do to pinpoint the exact information it is looking for is to examine the dependencies between the words. 
This kind of a rule-based algorithm is surprisingly powerful for general tasks in chatbots and other word-processing apps.

=== Why is it important?

Like in the example we discussed before, dependency parsing can play a really useful role in any application that tries to extract organized information from text. 
The dependency trees can also be used to identify "Subject-Verb-Object" triplets using the "nsubj" and "dobj" tags of the ROOT word, and this task is also called *Relation Extraction*.
Sometimes, the dependency relations can be converted into semantic tags/labels between the words, and this task is called *Semantic Role labelling*.

=== Why neural networks are much better at it?

=== Current state of the art methods and the available open source platforms 
Dependency parsing: spaCy and Huggingface transformers have been the most popular libraries for Dependency parsing, though Allen AI's parser is also catching up with their performance. 
We will experiment with a few of them below:

[source,python]
----
>>> import spacy
>>> nlp = spacy.load("en_core_web_sm")
>>> sentence = "We will be learning NLP today!"
>>> print ("{:<15} | {:<8} | {:<15} | {:<30} | {:<20}".format('Token','Relation','Head', 'Children', 'Meaning'))
>>> print ("-" * 115)

>>> for token in doc:
...     # Print the token, dependency nature, head, all dependents of the token, and meaning of the dependency
...     print ("{:<15} | {:<8} | {:<15} | {:<30} | {:<20}"
...             .format(str(token.text), str(token.dep_), str(token.head.text), str([child for child in token.children]) , str(spacy.explain(token.dep_))[:17] ))

Token           | Relation | Head            | Children                       | Meaning             
-------------------------------------------------------------------------------------------------
We              | nsubj    | learning        | []                             | nominal subject     
will            | aux      | learning        | []                             | auxiliary           
be              | aux      | learning        | []                             | auxiliary           
learning        | ROOT     | learning        | [We, will, be, NLP, today, !]  | root                
NLP             | dobj     | learning        | []                             | direct object       
today           | npadvmod | learning        | []                             | noun phrase as ad
!               | punct    | learning        | []                             | punctuation  

----

You can see above that every token's relation, syntactic head, syntactic children, and the meaning of the relation are printed out. 
The token "learning" has been assigned the tag of "ROOT". 
This is because in our sentence, the word "learning" happens to be the main verb when you organize it into a Subject-Verb-Object triple. 
Such verbs are called the ROOT verb, and they are the only tokens which do not have a syntactic head. 
You can use this library to extract clauses by separating the subtrees attached to the root by the relation of "advcl" or "relcl". 
You can also use it to extract relation triplets by identifying the tokens with "nsubj", "ROOT", and "dobj" dependencies.


Constituency parsing: Berkeley Neural Parser and Stanza have been the go-to options for the extraction of constituency relations in text. 
Let us explore them below:

1) Berkeley Neural Parser:
This parser cannot be used on its own, and requires either spaCy or NLTK to load it along with their existing models. 
We will discuss its usage with spaCy as that is the recommended way.
.Download the necessary packages
[source,python]
----
>>> import benepar
>>> benepar.download('benepar_en3')
----

After downloading the packages, we can test it out with a sample sentence. 
But we will be adding benepar to spaCy's pipeline first.

[source,python]
----
>>> import spacy
>>> nlp = spacy.load("en_core_web_md")
>>> if spacy.__version__.startswith('2'):
...     nlp.add_pipe(benepar.BeneparComponent("benepar_en3"))
... else:
...     nlp.add_pipe("benepar", config={"model": "benepar_en3"})
>>> doc = nlp("Johnson was compelled to ask the EU for an extension of the deadline, which was granted")
>>> sent = list(doc.sents)[0]
>>> print(sent._.parse_string)
(S (NP (NNP Johnson)) (VP (VBD was) (VP (VBN compelled) (S (VP (TO to) (VP (VB ask) (NP (DT the) (NNP EU)) (PP (IN for) (NP (NP (DT an) (NN extension)) (PP (IN of) (NP (NP (DT the) (NN deadline)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBD was) (VP (VBN granted)))))))))))))))
----

In the example above, we generated a parse string for the test sentence. The parse string includes various phrases and the POS tags of the tokens in the sentence. Some common tags you may notice in our parse string are NP ("Noun Phrase"), VP ("Verb Phrase"), S ("Sentence"), and PP ("Prepositional Phrase").
You can use this module to identify all the phrases in the sentence and use them in sentence simplification and/or summarization.

== Relation extraction
Relation extraction is the process of identifying connections between named entities in any text. 
Like Information extraction, it is classified into the categories closed and open.
In Closed relation extraction, the model extracts relations only from a given list of relation types. 
The advantages of this are that we can minimize the risk of getting untrue and bizarre relation labels between entities which makes us more confident about using them in real life. 
But the limitation is that it needs human labelers to come up with a list of relevant labels for every category of text, which as you can imagine, can get tedious and expensive.
In Open relation extraction, the model tries to come up with its own set of probable labels for the named entities in the text. 
This is suitable for processing large and generally unknown texts like Wikipedia articles and news entries. 

=== Current datasets and benchmarks
*1) TACRED*

The TAC Relation Extraction Dataset is a large scale dataset built with newswire and web text corpus. 
With over 100,000 examples, it covers 41 relation types which are organized into triplets. 
Over the past few years, efforts to address TACRED's limitations such as data quality and ambiguity in relation classes has given rise to datasets like Re-TACRED and DocRED.

*2) DocRED*

The Document Relation Extraction Dataset is the largest human-annotated dataset for document level relation extraction, where the model is required to go over multiple sentences in order to extract the relations between entities. 
Compiled using Wikidata and Wikipedia, this dataset is considered the de-facto benchmark for relation extraction methods along with TACRED due to its generalizability and size.

*3) SemEval Task-8 dataset*

The SemEval Task-8 dataset is a triplet extraction dataset with over 10,000 entries, each having one of 9 semantic relations between its entities. 
Though a much simpler dataset than TACRED and having only a few relation labels, this dataset is known for the quality of its sentence data and labels which is a big issue when it comes to TACRED, DocRED, and Re-TACRED.

=== Why is it important?
Relation extraction finds widespread application in finance and military, due to its significance in Information Extraction and Knowledge graph completion. 
Traditionally considered a triplet extraction task, relation extraction methods are now venturing beyond duplet and triplet relations and are finding extensive usage in medical industry in the form of drug combo extraction and hormone chain identification. 

=== Current state of the art methods and the available open source platforms 
Over the past few years, experiments with Deep Neural Networks have given strong results on triplet extraction and subsequently most of the research on the topic now follow neural methods. 
In this section, we will be discussing two recent neural relation extraction methods which have reported state of the art results on TACRED and DocRED.

*1) LUKE:*

TODO add description and code

*2) Typed entity markers*

The concept of Typed entity markers was developed as an improvement over LUKE and other neural relation extraction frameworks. 
In this method, typed markers are inserted before and after the entities in the text and fed into a multi-class classification model. 
Consider the example below:

Sentence:"John Smith works at Tangible AI"

Entities and their tags: John Smith (PERSON), Tangible AI (ORGANIZATION)

Sentence with typed entities: "^/PER/John Smith^ works at ^/ORG/Tangible AI^"

Following the example above, the sentence with typed entities is fed into the classification model with relations as its labels. 
As you may have guessed, NER is a necessary step before this process, for which we will be using spaCy as shown below:

[source,python]
----
>>> import spacy
>>> nlp = spacy.load("en_core_web_md")
>>> sent = "John Smith works at Tangible AI"
>>> doc = nlp(sent)
>>> entities = []
>>> for ent in doc.ents:
...     sent = sent.replace(ent.text, "^/" + ent.label_ + "/" + ent.text + "^")
>>> print(sent)
^/PER/John Smith^ works at ^/ORG/Tangible AI^

----

== Coreference resolution
Imagine you're running NER on a text, and you obtain the list of entities that the model has recognized. 
On closer inspection, you realize over half of them are duplicates because they're referring to the same terms! 
This is where *Coreference resolution* comes in handy because it identifies all the mentions of a noun in a sentence, helping us keep a track of all the pronouns and avoid multiple metions.

=== Current datasets and benchmarks 

*1) Ontonotes 5.0:*
This dataset is a compilation of various corpora of text(news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) with annotations of the named entities and noun phrases and their mentions. 
Available in three languages(English, Chinese, and Arabic), this dataset is the de facto benchmark for identifying coreferences in the industry.


*2) Winograd schema challenge:*
Consider this sentence- "The city councilmen refused the demonstrators a permit because they feared violence". 
Who does "they" in the sentence refer to? 
Our common sense tells us that it refers to the "city councilmen" and the answer seems to be easy for us, but this task of identifying mentions using common sense is surprisingly difficult for deep learning models. 
This task is called the Winograd schema challenge, also framed as "Commonsense reasoning" or "Commonsense inference" problem.

=== Why is it important?
Duplicate mentions is a big problem not only in *NER*, but *Relation extraction*, *Information extraction*, *Semantic parsing*, and many other tasks. 
Resolving all the pronouns saves the time and effort to extract the information associated with them. 

Moreover, it also helps us identify which entity or term is being talked about the most in a text, helping us assign importance to certain words over others. 
This technique has been experimented in *Topic modelling* and in constructing *knowledge graphs*.


=== Current state of the art methods and the available open source platforms 
1) spaCy and NeuralCoref

NeuralCoref 4.0 is currently the fastest entity resolver available open-source. 
It can be used as an extension to spaCy, as shown below: 

[source,python]
----
>>> import spacy
>>> nlp = spacy.load('en_core_web_md')
>>>
>>> import neuralcoref
>>> neuralcoref.add_to_pipe(nlp)
>>>
>>> doc = nlp(u'My sister has a dog. She loves him.')
>>>
>>> doc._.coref_clusters

----

On running the code above, you'll get a list of indices in an array. 
These are the indices of the words which the model identifies to be mentionings of the same noun phrases.


2) AllenNLP's Entity resolver

AllenNLP also provides a highly effective open source pipeline for Coreference resolution, though it is known to be much slower compared to NeuralCoref has a high memory requirement. 
Let us see how it works:

[source,python]
----
>>> from allennlp.predictors.predictor import Predictor
>>> import allennlp_models.tagging
>>>
>>> predictor = Predictor.from_path("https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz")
>>> predictor.predict(
    document="Paul Allen was born on January 21, 1953, in Seattle, Washington, to Kenneth Sam Allen and Edna Faye Allen. Allen attended Lakeside School, a private school in Seattle, where he befriended Bill Gates, two years younger, with whom he shared an enthusiasm for computers."
    )
>>>
----

== Information extraction

So you've learned that "information extraction" is converting unstructured text into structured information stored in a knowledge base or knowledge graph.
Information extraction is part of an area of research called natural language understanding (NLU), though that term is often used synonymously with natural language processing (NLP).

Information extraction and NLU is a different kind of learning than you may think of when researching data science.
It isn't only unsupervised learning; even the very "model" itself, the logic about how the world works, can be composed without human intervention.
Instead of giving your machine fish (facts), you're teaching it how to fish (extract information).
Nonetheless, machine learning techniques are often used to train the information extractor.

== Regular patterns

You need a pattern-matching algorithm that can identify sequences of characters or words that match the pattern so you can "extract" them from a longer string of text.
The easiest way to build such a pattern-matching algorithm is in Python, with a sequence of if/then statements that look for that symbol (a word or character) at each position of a string.
Say you wanted to find some common greeting words, such as "Hi", "Hello", and "Yo", at the beginning of a statement. You might do it something like this:

.Pattern hardcoded in Python
[source,python]
----
>>> def find_greeting(s):
...     """ Return greeting str (Hi, etc) if greeting pattern matches """
...     if s[0] == 'H':
...         if s[:3] in ['Hi', 'Hi ', 'Hi,', 'Hi!']:
...             return s[:2]
...         elif s[:6] in ['Hello', 'Hello ', 'Hello,', 'Hello!']:
...             return s[:5]
...     elif s[0] == 'Y':
...         if s[1] == 'o' and s[:3] in ['Yo', 'Yo,', 'Yo ', 'Yo!']:
...             return s[:2]
...     return None
----

And here's how it would work:

.Brittle pattern-matching example
[source,python]
----
>>> find_greeting('Hi Mr. Turing!')
'Hi'
>>> find_greeting('Hello, Rosa.')
'Hello'
>>> find_greeting("Yo, what's up?")
'Yo'
>>> find_greeting("Hello")
'Hello'
>>> print(find_greeting("hello"))
None
>>> print(find_greeting("HelloWorld"))
None
----

You can probably see how tedious programming a pattern matching algorithm this way would be.
And it's not even that good.
It's quite brittle, relying on precise spellings and capitalization and position characters in a string.
And it's tricky to specify all the "delimiters", such as punctuation, white space, or the beginnings and ends of strings (NULL characters) that are on either sides of words you're looking for.

You could probably come up with a way to allow you to specify different words or strings you want to look for without hard-coding them into Python expressions like this.
And you could even specify the delimiters in a separate function.
That would let you do some tokenization and iteration to find the occurrence of the words you're looking for anywhere in a string.
But that's a lot of work.

Fortunately that work has already been done!
A pattern-matching engine is integrated into most modern computer languages, including Python.
It's called regular expressions.
Regular expressions, such as string interpolation formatting expressions (for example, `"{:05d}".format(42)`), are a mini programming language unto themselves.
This language for pattern matching is called the regular expression language.
And Python has a regular expression interpreter (compiler and runner) in the standard library package `re`.
So let's use them to define your patterns instead of deeply nested Python `if` statements.

=== Regular expressions

Regular expressions are a strings written in a special computer language that you can use to specify algorithms.
Regular expressions are a lot more powerful, flexible, and concise than the equivalent Python you'd need to write to match patterns like this.
So regular expressions are the pattern definition language of choice for many NLP problems involving pattern matching.
This NLP application is an extension of their original use for compiling and interpreting formal languages (computer languages).

Regular expressions define a _finite state machine_ or FSM -- a tree of "if-then" decisions about a sequence of symbols, such as the `find_greeting()` function in listing 11.1.
The symbols in the sequence are passed into the decision tree of the FSM one symbol at a time.
A finite state machine that operates on a sequence of symbols such as ASCII character strings, or a sequence of English words, is called a _grammar_.
They can also be called _formal grammars_ to distinguish them from natural language grammar rules you learned in elementary school.

In computer science and mathematics, the word "grammar" refers to the set of rules that determine whether or a sequence of symbols is a valid member of a language, often called a computer language or formal language.
And a computer language, or formal language, is the set of all possible statements that would match the formal grammar that defines that language.
That's kind of a circular definition, but that's the way mathematics works sometimes.
You probably want to review appendix B if you aren't familiar with basic regular expression syntax and symbols such as `r'.\*'` and `r'a-z'`.

=== Information extraction as ML feature extraction

So you're back where you started in chapter 1, where we first mentioned regular expressions.
But didn't you switch from "grammar-based" NLP approaches at the end of chapter 1 in favor of machine learning and data-driven approaches?
Why return to hard-coded (manually composed) regular expressions and patterns?
Because your statistical or data-driven approach to NLP has limits.

You want your machine learning pipeline to be able to do some basic things, such as answer logical questions, or perform actions such as scheduling meetings based on NLP instructions.
And machine learning falls flat here.
You rarely have a labeled training set that covers the answers to all the questions people might ask in natural language.
Plus, as you'll see here, you can define a compact set of condition checks (a regular expression) to extract key bits of information from a natural language string.
And it can work for a broad range of problems.

Pattern matching (and regular expressions) continue to be the state-of-the art approach for information extraction (more commonly called _information retrieval_).
Even with machine learning approaches to natural language processing, you need to do feature engineering.
You need to create bags of words or "embeddings" of words to try to reduce the nearly infinite possibilities of meaning in natural language text into a vector that a machine can process easily.
Information extraction is just another form of machine learning feature extraction from unstructured natural language data, such as creating a bag of words, or doing PCA on that bag of words.
And these patterns and features are still employed in even the most advanced natural language machine learning pipelines such as Google's Assistant, Siri, Amazon Alexa, and other state-of-the-art "bots."

Information extraction is used to find statements and information that you might want your chatbot to have "on the tip of its tongue."
Information extraction can be accomplished beforehand to populate a knowledge base of facts.
Alternatively, the required statements and information can be found on-demand, when the chatbot is asked a question or a search engine is queried.
When a knowledge base is built ahead of time, the data structure can be optimized to facilitate faster queries within larger domains of knowledge.
A prebuilt knowledge base enables the chatbot to respond quickly to questions about a wider range of information.
If information is retrieved in real-time, as the chatbot is being queried, this is often called "search."
Google and other search engines combine these two techniques, querying a knowledge graph (knowledge base) and falling back to text search if the necessary facts aren't found.
Many of the natural language grammar rules you learned in school can be encoded in a formal grammar designed to operate on words or symbols representing parts of speech.
And the English language can be thought of as the words and grammar rules that make up the language.
Or you can think of it as the set of all possible things you could say that would be recognized as valid statements by an English language speaker.

And that brings us to another feature of formal grammars and finite state machines that will come in handy for NLP.
Any formal grammar can be used by a machine in two ways:

* To recognize "matches" to that grammar
* To generate a new sequence of symbols

Not only can you use patterns (regular expressions) for extracting information from natural language, but you can also use them in a chatbot that wants to "say" things that match that pattern!
We show you how to do this with a package called `rstr` footnote:[See the web page titled "leapfrogdevelopment / rstr — Bitbucket" (https://bitbucket.org/leapfrogdevelopment/rstr/).] for some of your information extraction patterns here.

This formal grammar and finite state machine approach to pattern matching has some other awesome features.
A true finite state machine can be guaranteed to always run in finite time (to "halt").
It will always tell you whether you've found a match in your string or not.
It will never get caught in a perpetual loop... as long as you don't use some of the advanced features of regular expression engines that allow you to "cheat" and incorporate loops into your FSM.

So you'll stick to regular expressions that don't require these "look-back" or "look-ahead" cheats.
You'll make sure your regular expression matcher processes each character and moves ahead to the next character only if it matches -- sort of like a strict train conductor walking through the seats checking tickets.
If you don't have one, the conductor stops and declares that there's a problem, a mismatch, and he refuses to go on, or look ahead or behind you until he resolves the problem.
There are no "go backs" or "do overs" for train passengers, or for strict regular expressions.

== Information worth extracting

Some keystone bits of quantitative information are worth the effort of "hand-crafted" regular expressions:

* GPS locations
* Dates
* Prices
* Numbers

Other important pieces of natural language information require more complex patterns than are easily captured with regular expressions:

* Question trigger words
* Question target words
* Named entities

=== Extracting GPS locations

GPS locations are typical of the kinds of numerical data you'll want to extract from text using regular expressions.
GPS locations come in pairs of numerical values for latitude and longitude.
They sometimes also include a third number for altitude, or height above sea level, but you'll ignore that for now.
Let's just extract decimal latitude/longitude pairs, expressed in degrees.
This will work for many Google Maps URLs.
Though URLs are not technically natural language, they are often part of unstructured text data, and you'd like to extract this bit of information, so your chatbot can know about places as well as things.

Let's use your decimal number pattern from previous examples, but let's be more restrictive and make sure the value is within the valid range for latitude (\+/- 90 deg) and longitude (+/- 180 deg).
You can't go any farther north than the North Pole (+90 deg) or farther south than the South Pole (-90 deg).
And if you sail from Greenwich England 180 deg east (+180 deg longitude), you'll reach the date line, where you're also 180 deg west (-180 deg) from Greenwich.

.Regular expression for GPS coordinates
[source,python]
----
>>> import re
>>> lat = r'([-]?[0-9]?[0-9][.][0-9]{2,10})'
>>> lon = r'([-]?1?[0-9]?[0-9][.][0-9]{2,10})'
>>> sep = r'[,/ ]{1,3}'
>>> re_gps = re.compile(lat + sep + lon)

>>> re_gps.findall('http://...maps/@34.0551066,-118.2496763...')
[(34.0551066, -118.2496763)]

>>> re_gps.findall("https://www.openstreetmap.org/#map=10/5.9666/116.0566")
[('5.9666', '116.0566')]

>>> re_gps.findall("Zig Zag Cafe is at 45.344, -121.9431 on my GPS.")
[('45.3440', '-121.9431')]
----

Numerical data is pretty easy to extract, especially if the numbers are part of a machine-readable string.
URLs and other machine-readable strings put numbers such as latitude and longitude in a predictable order, format, and units to make things easy for us.

This pattern will still accept some out-of-this-world latitude and longitude values, but it gets the job done for most of the URLs you'll copy from mapping web apps such as OpenStreetMap.

But what about dates?
Will regular expressions work for dates?
What if you want your date extractor to work in Europe and the US, where the order of day/month is often reversed?

=== Extracting dates

Dates are a lot harder to extract than GPS coordinates.
Dates are a more natural language, with different dialects for expressing similar things.
In the US, Christmas 2017 is "12/25/17."
In Europe, Christmas 2017 is "25/12/17."
You could check the locale of your user and assume that they write dates the same way as others in their region.
But this assumption can be wrong.

So most date and time extractors try to work with both kinds of day/month orderings and just check to make sure it's a valid date.
This is how the human brain works when we read a date like that.
Even if you were an US English speaker and you were in Brussels around Christmas, you'd probably recognize "25/12/17" as a holiday, because there are only 12 months in the year.

This "duck-typing" approach that works in computer programming can work for natural language, too.
If it looks like a duck and acts like a duck, it's probably a duck.
If it looks like a date and acts like a date, it's probably a date.
You'll use this "try it and ask forgiveness later" approach for other natural language processing tasks as well.
You'll try a bunch of options and accept the one the works.
You'll try your extractor or your generator, and then you'll run a validator on it to see if it makes sense.

For chatbots this is a particularly powerful approach, allowing you to combine the best of multiple natural language generators.
In chapter 10 you generated some chatbot replies using LSTMs.
To improve the user experience, you could generate a lot of replies and choose the one with the best spelling, grammar, and sentiment.
We'll talk more about this in chapter 12.

.Regular expression for US dates
[source,python]
----
>>> us = r'((([01]?\d)[-/]([0123]?\d))([-/]([0123]\d)\d\d)?)'
>>> mdy = re.findall(us, 'Santa came 12/25/2017. An elf appeared 12/12.')
>>> mdy
[('12/25/2017', '12/25', '12', '25', '/2017', '20'),
 ('12/12', '12/12', '12', '12', '', '')]
----

A list comprehension can be used to provide a little structure to that extracted data, by converting the month, day, and year into integers and labeling that numerical information with a meaningful name.

.Structuring extracted dates
[source,python]
----
>>> dates = [{'mdy': x[0], 'my': x[1], 'm': int(x[2]), 'd': int(x[3]),
...     'y': int(x[4].lstrip('/') or 0), 'c': int(x[5] or 0)} for x in mdy]
>>> dates
[{'mdy': '12/25/2017', 'my': '12/25', 'm': 12, 'd': 25, 'y': 2017, 'c': 20},
 {'mdy': '12/12', 'my': '12/12', 'm': 12, 'd': 12, 'y': 0, 'c': 0}]
----

Even for these simple dates, it's not possible to design a regex that can resolve all the ambiguities in the second date, "12/12."
There are ambiguities in the language of dates that only humans can guess at resolving using knowledge about things like Christmas and the intent of the writer of a text.
For examle "12/12" could mean:

* December 12th, 2017 -- month/day in the estimated year based on anaphora resolution footnote:[Issues in Anaphora Resolution
by Imran Q. Sayed for Stanford's CS224N course: https://nlp.stanford.edu/courses/cs224n/2003/fp/iqsayed/project_report.pdf .]
* December 12th, 2018 -- month/day in the current year at time of publishing
* December 2012 -- month/day in the


Because month/day come before the year in US dates and in our regex, '12/12' is presumed to be December 12th of an unknown year.
You can fill in any missing numerical fields with the most recently read year using the "context" from the structured data in memory:

.Basic context maintenance
[source,python]
----
>>> for i, d in enumerate(dates):
...     for k, v in d.items():
...         if not v:
...             d[k] = dates[max(i - 1, 0)][k]  # <1>
>>> dates
[{'mdy': '12/25/2017', 'my': '12/25', 'm': 12, 'd': 25, 'y': 2017, 'c': 20},
 {'mdy': '12/12', 'my': '12/12', 'm': 12, 'd': 12, 'y': 2017, 'c': 20}]
>>> from datetime import date
>>> datetimes = [date(d['y'], d['m'], d['d']) for d in dates]
>>> datetimes
[datetime.date(2017, 12, 25), datetime.date(2017, 12, 12)]
----
<1> This works because both the `dict` and the `list` are mutable data types.


This is a basic but reasonably robust way to extract date information from natural language text.
The main remaining tasks to turn this into a production date extractor would be to add some exception catching and context maintenance that is appropriate for your application.
If you added that to the `nlpia` package (http://github.com/totalgood/nlp) with a PR I'm sure your fellow readers would appreciate it.
And if you added some extractors for times, well, then you'd be quite the hero.

There are opportunities for some hand-crafted logic to deal with edge cases and natural language names for months and even days.
But no amount of sophistication could resolve the ambiguity in the date "12/11."
That could be

* December 11th in whatever year you read or heard it
* November 12th if you heard it in London or Launceston, Tasmania (a commonwealth territory)
* December 2011 if you read it in a US newspaper
* November 2012 if you read it in an EU newspaper

Some natural language ambiguities can't be resolved, even by a human brain.
But let's just make sure your date extractor can handle European day/month order by reversing month and day in your regex.

.Regular expression for European dates
[source,python]
----
>>> eu = r'((([0123]?\d)[-/]([01]?\d))([-/]([0123]\d)?\d\d)?)'
>>> dmy = re.findall(eu, 'Alan Mathison Turing OBE FRS (23/6/1912-7/6/1954) \
...     was an English computer scientist.')
>>> dmy
[('23/6/1912', '23/6', '23', '6', '/1912', '19'),
 ('7/6/1954', '7/6', '7', '6', '/1954', '19')]
>>> dmy = re.findall(eu, 'Alan Mathison Turing OBE FRS (23/6/12-7/6/54) \
...     was an English computer scientist.')
>>> dmy
[('23/6/12', '23/6', '23', '6', '/12', ''),
 ('7/6/54', '7/6', '7', '6', '/54', '')]
----

That regular expression correctly extracts Turing's birth and wake dates from a Wikipedia excerpt.
But I cheated, I converted the month "June" into the number 6 before testing the regular expression on that Wikipedia sentence.
So this isn't a realistic example.
And you'd still have some ambiguity to resolve for the year if the century is not specified.
Does the year `54` mean `1954` or does it mean `2054`?
You'd like your chatbot to be able to extract dates from unaltered Wikipedia articles so it can read up on famous people and learn import dates.
For your regex to work on more natural language dates, such as those found in Wikipedia articles, you need to add words such as "June" (and all its abbreviations) to your date-extracting regular expression.

You don't need any special symbols to indicate words (characters that go together in sequence).
You can just type them in the regex exactly as you'd like them to be spelled in the input, including capitalization.
All you have to do is put an `OR` symbol (`|`) between them in the regular expression.
And you need to make sure it can handle US month/day order as well as the European order.
You'll add these two alternative date "spellings" to your regular expression with a "big" OR (`|`) between them as a fork in your tree of decisions in the regular expression.

Let's use some named groups to help you recognize years such as "'84" as 1984 and "08" as 2008.
And let's try to be a little more precise about the 4-digit years you want to match, only matching years in the future up to 2399 and in the past back to year 0.footnote:[See the web page titled "Year zero - Wikipedia" (https://en.wikipedia.org/wiki/Year_zero).]

.Recognizing years
[source,python]
----
>>> yr_19xx = (
...     r'\b(?P<yr_19xx>' +
...     '|'.join('{}'.format(i) for i in range(30, 100)) +
...     r')\b'
...     )  # <1>
>>> yr_20xx = (
...     r'\b(?P<yr_20xx>' +
...     '|'.join('{:02d}'.format(i) for i in range(10)) + '|' +
...     '|'.join('{}'.format(i) for i in range(10, 30)) +
...     r')\b'
...     )  # <2>
>>> yr_cent = r'\b(?P<yr_cent>' + '|'.join(
...     '{}'.format(i) for i in range(1, 40)) + r')'  # <3>
>>> yr_ccxx = r'(?P<yr_ccxx>' + '|'.join(
...     '{:02d}'.format(i) for i in range(0, 100)) + r')\b'  # <4>
>>> yr_xxxx = r'\b(?P<yr_xxxx>(' + yr_cent + ')(' + yr_ccxx + r'))\b'
>>> yr = (
...     r'\b(?P<yr>' +
...     yr_19xx + '|' + yr_20xx + '|' + yr_xxxx +
...     r')\b'
...     )
>>> groups = list(re.finditer(
...     yr, "0, 2000, 01, '08, 99, 1984, 2030/1970 85 47 `66"))
>>> full_years = [g['yr'] for g in groups]
>>> full_years
['2000', '01', '08', '99', '1984', '2030', '1970', '85', '47', '66']
----
<1> 2-digit years 30-99 => 1930-1999
<2> 1- or 2-digit years 01-30 => 2001-2030
<3> First digits of a 3- or 4-digit yr such as the "1" in "123 A.D." or "20" in "2018"
<4> Last 2 digits of a 3- or 4-digit yr such as the "23" in "123 A.D." or "18" in "2018"

Wow!
That's a lot of work, just to handle some simple year rules in regex rather than in Python.
Don't worry, packages are available for recognizing common date formats.
They are much more precise (fewer false matches) and more general (fewer misses).
So you don't need to be able to compose complex regular expressions such as this yourself.
This example just gives you a pattern in case you need to extract a particular kind of number using a regular expression in the future.
Monetary values and IP addresses are examples where a more complex regular expression, with named groups, might come in handy.

Let's finish up your regular expression for extracting dates by adding patterns for the month names such as "June" or "Jun" in Turing's birthday on Wikipedia dates.

.Recognizing month words with regular expressions
[source,python]
----
>>> mon_words = 'January February March April May June July ' \
...     'August September October November December'
>>> mon = (r'\b(' + '|'.join('{}|{}|{}|{}|{:02d}'.format(
...     m, m[:4], m[:3], i + 1, i + 1) for i, m in enumerate(mon_words.split())) +
...     r')\b')
>>> re.findall(mon, 'January has 31 days, February the 2nd month of 12, has 28, except in a Leap Year.')
['January', 'February', '12']
----

Can you see how you might combine these regular expressions into a larger one that can handle both EU and US date formats?
One complication is that you can't reuse the same name for a group (parenthesized part of the regular expression).
So you can't just put an OR between the US and EU ordering of the named regular expressions for month and year.
And you need to include patterns for some optional separators between the day, month, and year.

Here's one way to do all that.

.Combining information extraction regular expressions
[source,python]
----
>>> day = r'|'.join('{:02d}|{}'.format(i, i) for i in range(1, 32))
>>> eu = (r'\b(' + day + r')\b[-,/ ]{0,2}\b(' +
...     mon + r')\b[-,/ ]{0,2}\b(' + yr.replace('<yr', '<eu_yr') + r')\b')
>>> us = (r'\b(' + mon + r')\b[-,/ ]{0,2}\b(' +
...     day + r')\b[-,/ ]{0,2}\b(' + yr.replace('<yr', '<us_yr') + r')\b')
>>> date_pattern = r'\b(' + eu + '|' + us + r')\b'
>>> list(re.finditer(date_pattern, '31 Oct, 1970 25/12/2017'))
[<_sre.SRE_Match object; span=(0, 12), match='31 Oct, 1970'>,
 <_sre.SRE_Match object; span=(13, 23), match='25/12/2017'>]
----

Finally, you need to validate these dates by seeing if they can be turned into valid Python `datetime` objects.

.Validating dates
[source,python]
----
>>> import datetime
>>> dates = []
>>> for g in groups:
...     month_num = (g['us_mon'] or g['eu_mon']).strip()
...     try:
...         month_num = int(month_num)
...     except ValueError:
...         month_num = [w[:len(month_num)]
...             for w in mon_words].index(month_num) + 1
...     date = datetime.date(
...         int(g['us_yr'] or g['eu_yr']),
...         month_num,
...         int(g['us_day'] or g['eu_day']))
...     dates.append(date)
>>> dates
[datetime.date(1970, 10, 31), datetime.date(2017, 12, 25)]
----

Your date extractor appears to work OK, at least for a few simple, unambiguous dates.
Think about how packages such as `Python-dateutil` and `datefinder` are able to resolve ambiguities and deal with more "natural" language dates such as "today" and "next Monday."
And if you think you can do it better than these packages, send them a pull request!

If you just want a state of the art date extractor, statistical (machine learning) approaches will get you there faster.
The Stanford Core NLP SUTime library (https://nlp.stanford.edu/software/sutime.html) and `dateutil.parser.parse` by Google are the state of the art.

== Extracting relationships (relations)

So far you've looked only at extracting tricky noun instances such as dates and GPS latitude and longitude values.
And you've worked mainly with numerical patterns.
It's time to tackle the harder problem of extracting knowledge from natural language.
You'd like your bot to learn facts about the world from reading an encyclopedia of knowledge such as Wikipedia.
You'd like it to be able to relate those dates and GPS coordinates to the entities it reads about.

What knowledge could your brain extract from this sentence from Wikipedia:

_On March 15, 1554, Desoto wrote in his journal that the Pascagoula people ranged as far north as the confluence of the Leaf and Chickasawhay rivers at 30.4, -88.5._

Extracting the dates and the GPS coordinates might enable you to associate that date and location with Desoto, the Pascagoula people, and two rivers whose names you can't pronounce.
You'd like your bot (and your mind) to be able to connect those facts to larger facts -- for example, that Desoto was a Spanish conquistador and that the Pascagoula people were a peaceful native American tribe.
And you'd like the dates and locations to be associated with the right "things": Desoto, and the intersection of two rivers, respectively.

This is what most people think of when they hear the term natural language understanding.
To understand a statement you need to be able to extract key bits of information and correlate it with related knowledge.
For machines, you store that knowledge in a graph, also called a knowledge base.
The edges of your knowledge graph are the relationships between things.
And the nodes of your knowledge graph are the nouns or objects found in your corpus.

The pattern you're going to use to extract these relationships (or relations) is a pattern such as SUBJECT - VERB - OBJECT.
To recognize these patterns, you'll need your NLP pipeline to know the parts of speech (POS) for each word in a sentence.

=== POS tagging

POS tagging can be accomplished with language models that contain dictionaries of words with all their possible parts of speech.
They can then be trained on properly tagged sentences to recognize the parts of speech in new sentences with other words from that dictionary.
NLTK and spaCy both implement POS tagging functions.
You'll use spaCy here because it is faster and more accurate.

.POS tagging with spaCy
[source,python]
----
>>> import spacy
>>> en_model = spacy.load('en_core_web_md')
>>> sentence = ("In 1541 Desoto wrote in his journal that the Pascagoula people " +
...     "ranged as far north as the confluence of the Leaf and Chickasawhay rivers at 30.4, -88.5.")
>>> parsed_sent = en_model(sentence)
>>> parsed_sent.ents
(1541, Desoto, Pascagoula, Leaf, Chickasawhay, 30.4)  # <1>

>>> ' '.join(['{}_{}'.format(tok, tok.tag_) for tok in parsed_sent])
'In_IN 1541_CD Desoto_NNP wrote_VBD in_IN his_PRP$ journal_NN that_IN the_DT Pascagoula_NNP people_NNS
 ranged_VBD as_RB far_RB north_RB as_IN the_DT confluence_NN of_IN the_DT Leaf_NNP and_CC Chickasawhay_NNP
 rivers_VBZ at_IN 30.4_CD ,_, -88.5_NFP ._.'  # <2>
----
<1> spaCy misses the longitude in the lat, lon numerical pair.
<2> spaCy uses the "OntoNotes 5" POS tags: https://spacy.io/api/annotation#pos-tagging

So to build your knowledge graph, you just need to figure out which objects (noun phrases) should be paired up.
You'd like to pair up the date "March 15, 1554" with the "named entity" Desoto.
You could then normalize those two strings (noun phrases) to point to objects you have in your knowledge base.
March 15, 1554 can be converted to a `datetime.date` object with a normalized representation.

spaCy-parsed sentences also contain the dependency tree in a nested dictionary.
And `spacy.displacy` can generate an _scalable vector graphics_ SVG string (or a complete HTML page), which can be viewed as an image in a browser.
This visualization can help you find ways to use the tree to create tag patterns for relation extraction.

.Visualize a dependency tree
[source,python]
----
>>> from spacy.displacy import render
>>> sentence = "In 1541 Desoto wrote in his journal about the Pascagoula."
>>> parsed_sent = en_model(sentence)
>>> with open('pascagoula.html', 'w') as f:
...     f.write(render(docs=parsed_sent, page=True, options=dict(compact=True)))
----

The dependency tree for this short sentence shows that the noun phrase "the Pascagoula" is the object of the relationship "met" for the subject "Desoto" (see figure 11.2).
And both nouns are tagged as proper nouns.

.The Pascagoula people
image::../images/ch11/pascagoula.jpg[Dependency tree for sentence about the Pascagoula people, width=80%, link="../images/ch11/pascagoula.jpg"]

To create POS and word property patterns for a `spacy.matcher.Matcher`, listing all the token tags in a table is helpful.
Here are some helper functions to make that easier:

.Helper functions for spaCy tagged strings
[source,python]
----
>>> import pandas as pd
>>> from collections import OrderedDict

>>> def token_dict(token):
...     return OrderedDict(ORTH=token.orth_, LEMMA=token.lemma_,
...         POS=token.pos_, TAG=token.tag_, DEP=token.dep_)

>>> def doc_dataframe(doc):
...     return pd.DataFrame([token_dict(tok) for tok in doc])

>>> doc_dataframe(en_model("In 1541 Desoto met the Pascagoula."))
         ORTH       LEMMA    POS  TAG    DEP
0          In          in    ADP   IN   prep
1        1541        1541    NUM   CD   pobj
2      Desoto      desoto  PROPN  NNP  nsubj
3         met        meet   VERB  VBD   ROOT
4         the         the    DET   DT    det
5  Pascagoula  pascagoula  PROPN  NNP   dobj
6           .           .  PUNCT    .  punct
----

Now you can see the sequence of POS or TAG features that will make a good pattern.
If you're looking for "has-met" relationships between people and organizations, you'd probably like to allow patterns such as "PROPN met PROPN", "PROPN met the PROPN", "PROPN met with the PROPN", and "PROPN often meets with PROPN".
You could specify each of those patterns individually, or try to capture them all with some * or ? operators on "any word" patterns between your proper nouns:

[source,]
----
'PROPN ANYWORD? met ANYWORD? ANYWORD? PROPN'
----

Patterns in spaCy are much more powerful and flexible than the preceding pseudocode, so you have to be more verbose to explain exactly the word features you'd like to match.
In a spaCy pattern specification you use a dictionary to capture all the tags that you want to match for each token or word.

[source,python]
.Example spaCy POS pattern
----
>>> pattern = [{'TAG': 'NNP', 'OP': '+'}, {'IS_ALPHA': True, 'OP': '*'},
...            {'LEMMA': 'meet'},
...            {'IS_ALPHA': True, 'OP': '*'}, {'TAG': 'NNP', 'OP': '+'}]
----

You can then extract the tagged tokens you need from your parsed sentence.

.Creating a POS pattern matcher with spaCy
[source,python]
----
>>> from spacy.matcher import Matcher
>>> doc = en_model("In 1541 Desoto met the Pascagoula.")
>>> matcher = Matcher(en_model.vocab)
>>> matcher.add('met', None, pattern)
>>> m = matcher(doc)
>>> m
[(12280034159272152371, 2, 6)]

>>> doc[m[0][1]:m[0][2]]
Desoto met the Pascagoula
----

So you extracted a match from the original sentence from which you created the pattern, but what about similar sentences from Wikipedia?

.Using a POS pattern matcher
[source,python]
----
>>> doc = en_model("October 24: Lewis and Clark met their" \
...                "first Mandan Chief, Big White.")
>>> m = matcher(doc)[0]
>>> m
(12280034159272152371, 3, 11)

>>> doc[m[1]:m[2]]
Lewis and Clark met their first Mandan Chief

>>> doc = en_model("On 11 October 1986, Gorbachev and Reagan met at Höfði house")
>>> matcher(doc)
[]  # <1>
----
<1> The pattern doesn't match any substrings of the sentence from Wikipedia.

You need to add a second pattern to allow for the verb to occur after the subject and object nouns.

.Combining multiple patterns for a more robust pattern matcher
[source,python]
----
>>> doc = en_model("On 11 October 1986, Gorbachev and Reagan met at Hofoi house")
>>> pattern = [{'TAG': 'NNP', 'OP': '+'}, {'LEMMA': 'and'},
...            {'TAG': 'NNP', 'OP': '+'},
...            {'IS_ALPHA': True, 'OP': '*'}, {'LEMMA': 'meet'}]
>>> matcher.add('met', None, pattern)  # <1>
>>> m = matcher(doc)
>>> m
[(14332210279624491740, 5, 9),
 (14332210279624491740, 5, 11),
 (14332210279624491740, 7, 11),
 (14332210279624491740, 5, 12)]  <2>

>>> doc[m[-1][1]:m[-1][2]]  <3>
Gorbachev and Reagan met at Hofoi house
----
<1> This adds an additional pattern without removing the previous pattern.
<2> The '+' operators increase the number of overlapping alternative matches.
<3> The longest match is the last one in the list of matches.

So now you have your entities and a relationship.
You can even build a pattern that is less restrictive about the verb in the middle ("met") and more restrictive about the names of the people and groups on either side.
Doing so might allow you to identify additional verbs that imply that one person or group has met another, such as the verb "knows" or even passive phrases such as "had a conversation" or "became acquainted with".
Then you could use these new verbs to add relationships for new proper nouns on either side.

But you can see how you're drifting away from the original meaning of your seed relationship patterns.
This is called semantic drift.
Fortunately, spaCy tags words in a parsed document with not only their POS and dependency tree information, but it also provides the Word2Vec word vector.
You can use this vector to prevent the connector verb and the proper nouns on either side from drifting too far away from the original meaning of your seed pattern.footnote:[This is the subject of active research: https://nlp.stanford.edu/pubs/structuredVS.pdf.]

=== Entity name normalization

The normalized representation of an entity is usually a string, even for numerical information such as dates.
The normalized ISO format for this date would be "1541-01-01".
A normalized representation for entities enables your knowledge base to connect all the different things that happened in the world on that same date to that same node (entity) in your graph.

You'd do the same for other named entities.
You'd correct the spelling of words and attempt to resolve ambiguities for names of objects, animals, people, places, and so on.
Normalizing named entities and resolving ambiguities is often called "coreference resolution" or "anaphora resolution", especially for pronouns or other "names" relying on context.
This is similar to lemmatization, which we discussed in chapter 2.
Normalization of named entities ensures that spelling and naming variations don't pollute your vocabulary of entity names with confounding, redundant names.

For example "Desoto" might be expressed in a particular document in at least five different ways:

* "de Soto"
* "Hernando de Soto"
* "Hernando de Soto (c. 1496/1497–1542), Spanish conquistador"
* https://en.wikipedia.org/wiki/Hernando_de_Soto (a URI)
* A numerical ID for a database of famous and historical people

Similarly your normalization algorithm can choose any of these forms.
A knowledge graph should normalize each kind of entity the same way, to prevent multiple distinct entities of the same type from sharing the same "name."
You don't want multiple person names referring to the same physical person.
Even more importantly, the normalization should be applied consistently -- both when you write new facts to the knowledge base or when you read or query the knowledge base.

If you decide to change the normalization approach after the database has been populated, the data for existing entities in the knowledge should be "migrated", or altered, to adhere to the new normalization scheme.
Schemaless databases (key-value stores), like the ones used to store knowledge graphs or knowledge bases, are not free from the migration responsibilities of relational databases.
After all, schemaless databases are interface wrappers for relational databases under the hood.

Your normalized entities also need "is-a" relationships to connect them to entity categories that define types or categories of entities.
These "is-a" relationships can be thought of as tags because each entity can have multiple "is-a" relationships.
Like names of people or POS tags, dates and other discrete numerical objects need to be normalized if you want to incorporate them into your knowledge base.

What about _relations_ between entities -- do they need to be stored in some normalized way?

=== Relation normalization and extraction

Now you need to a way to normalize the relationships, to identify the kind of relationship between entities.
Doing so will allow you to find all birthday relationships between dates and people, or dates of occurrences of historical events, such as the encounter between "Hernando de Soto" and the "Pascagoula people."
And you need to write an algorithm to choose the right label for your relationship.

And these relationships can have a hierarchical name, such as "occurred-on/approximately" and "occurred-on/exactly", to allow you to find specific relationships or categories of relationships.
You can also label these relationships with a numerical property for the "confidence", probability, weight, or normalized frequency (analogous to TF-IDF for terms/words) of that relationship.
You can adjust these confidence values each time a fact extracted from a new text corroborates or contradicts an existing fact in the database.

Now you need a way to match patterns that can find these relationships.

=== Word patterns

Word patterns are just like regular expressions, but for words instead of characters.
Instead of character classes, you have word classes.
For example, instead of matching a lowercase character you might have a word pattern decision to match all the singular nouns ("NN" POS tag).footnote:[spaCy uses the "OntoNotes 5" POS tags: https://spacy.io/api/annotation#pos-tagging]
This is usually accomplished with machine learning.
Some seed sentences are tagged with some correct relationships (facts) extracted from those sentences.
A POS pattern can be used to find similar sentences where the subject and object words might change or even the relationship words.

You can use the spaCy package two different ways to match these patterns in latexmath:[O(1)] (constant time) no matter how many patterns you want to match:

* PhraseMatcher for any word/tag sequence patterns footnote:[See the web page titled "Code Examples : spaCy Usage Documentation" (https://spacy.io/usage/examples#phrase-matcher).]
* Matcher for POS tag sequence patterns footnote:[See the web page titled "Matcher : spaCy API Documentation" (https://spacy.io/api/matcher).]

To ensure that the new relations found in new sentences are truly analogous to the original seed (example) relationships, you often need to constrain the subject, relation, and object word meanings to be similar to those in the seed sentences.
The best way to do this is with some vector representation of the meaning of words.
Does this ring a bell?
Word vectors, discussed in chapter 4, are one of the most widely used word meaning representations for this purpose.
They help minimize semantic drift.

Using semantic vector representations for words and phrases has made automatic information extraction accurate enough to build large knowledge bases automatically.
But human supervision and curation is required to resolve much of the ambiguity in natural language text.
CMU's NELL (Never-Ending Language Learner)footnote:[See the web page titled "NELL: The Computer that Learns - Carnegie Mellon University" (https://www.cmu.edu/homepage/computing/2010/fall/nell-computer-that-learns.shtml).] enables users to vote on changes to the knowledge base using Twitter and a web application.

=== Segmentation

We've skipped one form of information extraction or tool used in information extraction.
Most of the documents you've used in this chapter have been bite-sized chunks containing just a few facts and named entities.
But in the real world you may need to create these chunks yourself.

Document "chunking" is useful for creating semi-structured data about documents that can make it easier to search, filter, and sort documents for information retrieval.
And for information extraction, if you're extracting relations to build a knowledge base such as NELL or Freebase, you need to break it into parts that are likely to contain a fact or two.
When you divide natural language text into meaningful pieces, it's called _segmentation_.
The resulting segments can be phrases, sentences, quotes, paragraphs, or even entire sections of a long document.

Sentences are the most common chunk for most information extraction problems.
Sentences are usually punctuated with one of a few symbols (".", "?", "!", or a new line).
And grammatically correct English language sentences must contain a subject (noun) and a verb, which means they'll usually have at least one relation or fact worth extracting.
And sentences are often self-contained packets of meaning that don't rely too much on preceding text to convey most of their information.

Fortunately most languages, including English, have the concept of a sentence, a single statement with a subject and verb that says something about the world.
Sentences are just the right bite-sized chunk of text for your NLP knowledge extraction pipeline.
For the chatbot pipeline, your goal is to segment documents into sentences, or statements.

In addition to facilitating information extraction, you can flag some of those statements and sentences as being part of a dialog or being suitable for replies in a dialog.
Using a sentence segmenter allows you to train your chatbot on longer texts, such as books.
Choosing those books appropriately gives your chatbot a more literary, intelligent style than if you trained it purely on Twitter streams or IRC chats.
And these books give your chatbot access to a much broader set of training documents to build its common sense knowledge about the world.

==== Sentence segmentation

Sentence segmentation is usually the first step in an information extraction pipeline.
It helps isolate facts from each other so that you can associate the right price with the right thing in a string such as "The Babel fish costs $42. 42 cents for the stamp."
And that string is a good example of why sentence segmentation is tough -- the dot in the middle could be interpreted as a decimal or a "full stop" period.

One of the simplest pieces of "information" you can extract from a document are sequences of words that contain a logically cohesive statement.

The most important segments in a natural language document, after words, are sentences.
Sentences contain a logically cohesive statement about the world.
These statements contain the information you want to extract from text.
Sentences often tell you the relationship between things and how the world works when they make statements of fact, so you can use sentences for knowledge extraction.
And sentences often explain when, where, and how things happened in the past, tend to happen in general, or will happen in the future.
So we should also be able to extract facts about dates, times, locations, people, and even sequences of events or tasks using sentences as our guide.
And, most importantly, all natural languages have sentences or logically cohesive sections of text of some sort.
And all languages have a widely shared process for generating them (a set of grammar "rules" or habits).

But segmenting text, identifying sentence boundaries is a bit trickier than you might think.
In English, for example, no single punctuation mark or sequence of characters always marks the end of a sentence.

=== Why won't `split('.!?')` work?

Even a human reader might have trouble finding an appropriate sentence boundary within each of the following quotes.
And if they did find multiple sentences from each, they would be wrong for four out of five of these difficult examples:

_I live in the U.S. but I commute to work in Mexico on S.V. Australis for a woman from St. Bernard St. on the Gulf of Mexico._

_I went to G.T.You?_

_She yelled "It's right here!" but I kept looking for a sentence boundary anyway._

_I stared dumbfounded on as things like "How did I get here?", "Where am I?", "Am I alive?" flittered across the screen._

_The author wrote "'I don't think it's conscious.' Turing said."_

Even a human reader might have trouble finding an appropriate sentence boundary within each of these quotes.

More sentence segmentation "edge cases" such as this are available at tm-town.com footnote:[See the web page titled "Natural Language Processing : TM-Town" (https://www.tm-town.com/natural-language-processing#golden_rules).] and within the nlpia.data module.

Technical text is particularly difficult to segment into sentences because engineers, scientists, and mathematicians tend to use periods and exclamation points to signify a lot of things besides the end of a sentence.
When we tried to find the sentence boundaries in this book, we had to manually correct several of the extracted sentences.

If only we wrote English like telegrams, with a "STOP" or unique punctuation mark at the end of each sentence.
But since we don't, you'll need some more sophisticated NLP than just `split('.!?')`.
Hopefully you're already imagining a solution in your head.
If so, it's probably based on one of the two approaches to NLP you've used throughout this book:

* Manually programmed algorithms (regular expressions and pattern-matching)
* Statistical models (data-based models or machine learning)

We use the sentence segmentation problem to revisit these two approaches by showing you how to use regular expressions as well as perceptrons to find sentence boundaries.
And you'll use the text of this book as a training and test set to show you some of the challenges.
Fortunately you haven't inserted any newlines within sentences, to manually "wrap" text like in newspaper column layouts.
Otherwise, the problem would be even more difficult.
In fact, much of the source text for this book, in ASCIIdoc format, has been written with "old-school" sentence separators (two spaces after the end of every sentence), or with each sentence on a separate line.
This was so we could use this book as a training and test set for your segmenters.

=== Sentence segmentation with regular expressions

Regular expressions are just a shorthand way of expressing the tree of "`if...then`" rules (regular grammar rules) for finding character patterns in strings of characters.
As we mentioned in chapters 1 and 2, regular expressions (regular grammars) are a particularly succinct way to specify the s of a finite state machine.
Our regex or FSM has only one purpose: Identify sentence boundaries.

If you do a web search for sentence segmenters,footnote:[See the web page titled "Python sentence segment at DuckDuckGo" (https://duckduckgo.com/?q=Python+sentence+segment&t=canonical&ia=qa).] you're likely to be pointed to various regular expressions intended to capture the most common sentence boundaries.
Here are some of them, combined and enhanced to give you a fast, general-purpose sentence segmenter.

The following regex would work with a few "normal" sentences.

[source,python]
>>> re.split(r'[!.?]+[ $]', "Hello World.... Are you there?!?! I'm going to Mars!")
['Hello World', 'Are you there', "I'm going to Mars!"]

Unfortunately, this `re.split` approach gobbles up the sentence-terminating token, and only retains it if it is the last character in a document or string.
But it does do a good job of ignoring the trickery of periods within doubly-nested quotes:

[source,python]
>>> re.split(r'[!.?] ', "The author wrote \"'I don't think it's conscious.' Turing said.\"")
['The author wrote "\'I don\'t think it\'s conscious.\' Turing said."']

It also ignores periods in quotes that terminate an actual sentence.
This can be a good thing or a bad thing, depending on your information extraction steps that follow your sentence segmenter.

[source,python]
>>> re.split(r'[!.?] ', "The author wrote \"'I don't think it's conscious.' Turing said.\" But I stopped reading.")
['The author wrote "\'I don\'t think it\'s conscious.\' Turing said." But I stopped reading."']

What about abbreviated text, such as SMS messages and tweets?
Sometimes hurried humans squish sentences together, leaving no space surrounding periods.
Alone, the following regex could only deal with periods in SMS messages that have letters on either side, and it would safely skip over numerical values:

[source,python]
>>> re.split(r'(?<!\d)\.|\.(?!\d)', "I went to GT.You?")
['I went to GT', 'You?']

Even combining these two regexes isn't enough to get more than a few right in the difficult test cases from `nlpia.data`.

[source,python]
----
>>> from nlpia.data.loaders import get_data
>>> regex = re.compile(r'((?<!\d)\.|\.(?!\d))|([!.?]+)[ $]+')
>>> examples = get_data('sentences-tm-town')
>>> wrong = []
>>> for i, (challenge, text, sents) in enumerate(examples):
...     if tuple(regex.split(text)) != tuple(sents):
...         print('wrong {}: {}{}'.format(i, text[:50], '...' if len(text) > 50 else ''))
...         wrong += [i]
>>> len(wrong), len(examples)
(61, 61)
----

You'd have to add a lot more "look-ahead" and "look-back" to improve the accuracy of a regex sentence segmenter.
A better approach for sentence segmentation is to use a machine learning algorithm (often a single-layer neural net or logistic regression) trained on a labeled set of sentences.
Several packages contain such a model you can use to improve your sentence segmenter:

* DetectorMorse footnote:[See the web page titled "GitHub - cslu-nlp/DetectorMorse: Fast supervised sentence boundary detection using the averaged perceptron" (https://github.com/cslu-nlp/detectormorse).]
* spaCy footnote:[See the web page titled "Facts & Figures : spaCy Usage Documentation" (https://spacy.io/usage/facts-figures).]
* SyntaxNet footnote:[See the web page titled "models/syntaxnet-tutorial.md at master" (https://github.com/tensorflow/models/blob/master/research/syntaxnet/g3doc/syntaxnet-tutorial.md).]
* NLTK (Punkt) footnote:[See the web page titled "nltk.tokenize package — NLTK 3.3 documentation" (http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt).]
* Stanford CoreNLP footnote:[See the web page titled "torotoki / corenlp-python — Bitbucket" (https://bitbucket.org/torotoki/corenlp-python).]

You use the spaCy sentence segmenter (built into the parser) for most of your mission-critical applications.
spaCy has few dependencies and compares well with the others on accuracy and speed.
DetectorMorse, by Kyle Gorman, is another good choice if you want state-of-the-art performance in a pure Python implementation that you can refine with your own training set.

== In the real world

Information extraction and question answering systems are used for:

* TA assistants for university courses
* Customer service
* Tech support
* Sales
* Software documentation and FAQs

Information extraction can be used to extract things such as:

* Dates
* Times
* Prices
* Quantities
* Addresses
* Names
** People
** Places
** Apps
** Companies
** Bots
* Relationships
** "is-a" (kinds of things)
** "has" (attributes of things)
** "related-to"

Whether information is being parsed from a large corpus or from user input on the fly, being able to extract specific details and store them for later use is critical to the performance of a chatbot.
First by identifying and isolating this information and then by tagging relationships between those pieces of information we have learned to "normalize" information programmatically.
With that knowledge safely shelved in a search-able structure, your chatbot will be equipped with the tools to hold its own in a conversation in a given domain.

=== Summary

* A knowledge graph can be built to store relationships between entities.
* Regular expressions are a mini-programming language that can isolate and extract information.
* Part-of-speech tagging allows you to extract relationships between entities mentioned in a sentence.
* Segmenting sentences requires more than just splitting on periods and exclamation marks.
